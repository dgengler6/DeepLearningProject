{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "greek-mechanism",
   "metadata": {},
   "source": [
    "## Usefuls sources of informations:\n",
    "\n",
    "* https://github.com/hbjornoy/deep-learning/blob/master/project2/helpers.py\n",
    "* https://github.com/Ertugrulmert/EPFL-Deep-Learning-Projects/blob/master/Project_2/framework.py\n",
    "* https://github.com/marieanselmet/DeepLearningEPFL_projects/tree/main/DL_framework_from_scratch\n",
    "\n",
    "La grande différence est que tout le monde définit module avec un argument, et non une liste * -> à voir si on doit changer pour faire comme les années précédentes ou s'il y a qqc de plus pratique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-worry",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "* Pour les modules retournant une empty list sur param, enlever la fonction param (ça sert à rien de l'override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "minimal-sewing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f8744b6eac0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from torch import Tensor\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stable-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The father of all Modules\n",
    "class Module(object):\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-packing",
   "metadata": {},
   "source": [
    "## Activation\n",
    "\n",
    "À part ReLU et Tanh c'est pas précisé dans l'énoncé - là c'est juste pour le flex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "devoted-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t = 0 \n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.t = input\n",
    "        return input.clamp(0)\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        input = self.t\n",
    "        sign = input.sign().clamp(0)\n",
    "        return sign * gradwrtoutput\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "juvenile-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t = 0 \n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.t = input\n",
    "        \n",
    "        out = []\n",
    "        for x in input :\n",
    "            e = math.exp(-2 * x)\n",
    "            val = (1 - e) / (1 + e)\n",
    "            out.append(val)\n",
    "        return torch.FloatTensor(out)\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        z = self.t\n",
    "        e = torch.exp(-2 * z)x\n",
    "        d_tanh = 4 * e / (1 + e)**2\n",
    "        return d_tanh * gradwrtoutput\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rapid-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t = 0 \n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.t = input\n",
    "        return input.mul(-1).exp().add(1).pow(-1)\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        sig = self.t.mul(-1).exp().add(1).pow(-1)\n",
    "        return sig.mul(-1).add(1).mul(sig)\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23a1e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  0.,  1.,  3., 10.])\n",
      "tensor([-0.,  0.,  1.,  3., 10.])\n",
      "tensor([-1.0000,  0.0000,  0.7616,  0.9951,  1.0000])\n",
      "tensor([-8.2446e-08,  0.0000e+00,  4.1997e-01,  2.9598e-02,  8.2446e-08])\n",
      "tensor([4.5398e-05, 5.0000e-01, 7.3106e-01, 9.5257e-01, 9.9995e-01])\n",
      "tensor([4.5396e-05, 2.5000e-01, 1.9661e-01, 4.5177e-02, 4.5417e-05])\n"
     ]
    }
   ],
   "source": [
    "r = ReLU()\n",
    "th = Tanh()\n",
    "s = Sigmoid()\n",
    "\n",
    "t = torch.FloatTensor([-10, 0, 1, 3, 10])\n",
    "\n",
    "print(r.forward(t))\n",
    "print(r.backward(t))\n",
    "print(th.forward(t))\n",
    "print(th.backward(t))\n",
    "print(s.forward(t))\n",
    "print(s.backward(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "passing-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "julian-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELU(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "damaged-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CReLU(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-romantic",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "public-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, input_size, output_size, mean=0, std=1):\n",
    "        super().__init__()\n",
    "        self.W = torch.empty(output_size, input_size).normal_(mean, std)\n",
    "        self.b = torch.empty(output_size).normal_(mean, std)\n",
    "        self.t = 0\n",
    "        self.dW = torch.zeros(output_size, input_size)\n",
    "        self.db = torch.zeros(output_size)\n",
    "    def forward(self, input):\n",
    "        self.t = input\n",
    "        return self.W.mv(input).add(self.b)\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        self.dW.add_(gradwrtoutput.view(1, -1).t().mm(self.t.view(1, -1)))\n",
    "        self.db.add_(gradwrtoutput)\n",
    "        return self.W.t().mv(gradwrtoutput)\n",
    "        \n",
    "        \n",
    "    def param(self):\n",
    "        return [(self.W, self.dW), (self.b, self.db)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a16320dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.6519,  5.7937, -1.7806,  0.9086, -0.8110, -1.6347,  3.7294,  0.6875,\n",
       "        -3.0259, -1.3251])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = Linear(10, 10)\n",
    "t = torch.empty(10).fill_(1)\n",
    "l.forward(t)\n",
    "l.backward(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49fa1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p = 0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, input):\n",
    "        for i in range(input.shape[0]):\n",
    "            if random.random() < self.p : \n",
    "                input[i] = 0 \n",
    "        return input\n",
    "    \n",
    "    def backward(self, gradwrtoutput):\n",
    "        return gradwrtoutput\n",
    "        \n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcf391b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 1., 0., 0., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = Dropout(0.5)\n",
    "t = torch.empty(10).fill_(1)\n",
    "l.forward(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-account",
   "metadata": {},
   "source": [
    "## Sequential\n",
    "\n",
    "Lui en input il prend une liste de modules et son fwd c'est de feed le fwd de chaque layer dans l'autre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dirty-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, module_list):\n",
    "        super().__init__()\n",
    "        self.modules = module_list\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for module in self.modules :\n",
    "            x = module.forward(x)\n",
    "        return x \n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        x = gradwrtoutput\n",
    "        for module in self.modules :\n",
    "            x = module.backward(x)\n",
    "        return x\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b904c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Sequential([Linear(10,10), Linear(10,5), Linear(5,2)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-representation",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "Seule lossMSE est demandé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ready-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        return (pred - target.float()).pow(2).mean()\n",
    "    \n",
    "    # J'ai pas mis les arguments dans backward car je pars du principe que la MSE est la dernière fonction utilisée\n",
    "    def backward(self):\n",
    "        return 2 * (self.pred - self.target)\n",
    "    \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cutting-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMAE(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        return (pred - target.float()).abs().mean()\n",
    "\n",
    "    def backward(self):\n",
    "        error = self.pred - self.target\n",
    "        return error.sign()\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "industrial-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross entropy loss\n",
    "# Note: We need to use sigmoid function before using BCE otherwise it makes no sense\n",
    "class LossBCE(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        print(target * pred.log())\n",
    "        return ((target * pred.log()) + ((1 - target) * (1 - pred).log())).mean()\n",
    "\n",
    "    def backward(self):\n",
    "        return ((-target/pred) + ((1 - target)/(1 - pred))).mean()\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "stretch-breath",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6667)\n",
      "tensor([-2.,  0.,  4.])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "target = torch.FloatTensor([1, 1, 1])\n",
    "pred = torch.FloatTensor([0, 1, 3])\n",
    "lossMSE = LossMSE()\n",
    "print(lossMSE.forward(pred, target))\n",
    "print(lossMSE.backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "simple-nation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor([-1.,  0.,  1.])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "target = torch.FloatTensor([1, 1, 1])\n",
    "pred = torch.FloatTensor([0, 1, 3])\n",
    "lossMAE = LossMAE()\n",
    "print(lossMAE.forward(pred, target))\n",
    "print(lossMAE.backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "clinical-nashville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0000, -0.0000, -0.5108, -2.3026])\n",
      "tensor(-0.8189)\n",
      "tensor(-2.2817)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "target = torch.FloatTensor([0, 0, 1, 1])\n",
    "pred = torch.FloatTensor([0.3, 0.1, 0.6, 0.1])\n",
    "lossBCE = LossBCE()\n",
    "print(lossBCE.forward(pred, target))\n",
    "print(lossBCE.backward())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-beast",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Pas demandé mais fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bacterial-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        for module in self.params:\n",
    "            for tup in module:\n",
    "                param, grad = tup\n",
    "                if (param is not None) and (grad is not None):\n",
    "                    param.add_(grad, alpha=self.lr)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for module in self.params:\n",
    "            for tup in module:\n",
    "                param, grad = tup\n",
    "                if (param is not None) and (grad is not None):\n",
    "                    grad.zero_()\n",
    "                    \n",
    "    def show(self):\n",
    "        # To remove, for debugging\n",
    "        for i in self.params:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "democratic-scholarship",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-20-3375e4669dd1>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-3375e4669dd1>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    class Adam():\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# Mazel Tov si on y arrive\n",
    "class Adam():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "proved-hostel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10.5000,   5.0000],\n",
      "        [  8.0000, -20.0000]])\n",
      "tensor([[  1.,  10.],\n",
      "        [ -1., -10.]])\n",
      "tensor([[10.5000,  0.0000],\n",
      "        [ 8.0000,  0.0000]])\n",
      "tensor([[ 1.,  0.],\n",
      "        [-1.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "params = torch.FloatTensor([[(10, 5), (10, -20)], [(0, 10), (0, -10)]])\n",
    "sgd = SGD(params , 0.1)\n",
    "sgd.step()\n",
    "sgd.show()\n",
    "sgd.zero_grad()\n",
    "sgd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-evening",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "experienced-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    input = torch.empty(nb, 2).uniform_(0, 1)\n",
    "    \n",
    "    distance = torch.norm((input - torch.Tensor([[0.5, 0.5]])), 2, 1, True)\n",
    "    target = distance.mul(math.sqrt(2*math.pi)).sub(1).sign().sub(1).div(-2).long()  \n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "greek-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pas sûr de l'interêt du one hot encoder vu l'énoncé mais je peux me tromper \n",
    "def one_hot_encoder(target):\n",
    "    return torch.empty(target.size(0), 2).zero_().scatter_(1, target.view(-1, 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "correct-vegetable",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4963, 0.7682],\n",
      "        [0.0885, 0.1320],\n",
      "        [0.3074, 0.6341],\n",
      "        ...,\n",
      "        [0.2335, 0.8050],\n",
      "        [0.5335, 0.5196],\n",
      "        [0.3481, 0.8767]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target = generate_disc_set(1000)\n",
    "print(input)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "static-convergence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-alfred",
   "metadata": {},
   "source": [
    "## Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "introductory-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"EKIP\")\n",
    "    \n",
    "def test():\n",
    "    print(667)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "downtown-candle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EKIP\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "coastal-spotlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "667\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-confidence",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "convenient-marine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faisons de la merde ici\n"
     ]
    }
   ],
   "source": [
    "print(\"Faisons de la merde ici\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-elder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
