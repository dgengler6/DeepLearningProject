{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe89fd1",
   "metadata": {},
   "source": [
    "## Deep Learning Project 1 \n",
    "\n",
    "Comparing digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb1292",
   "metadata": {},
   "source": [
    "### TODO Next : \n",
    "\n",
    "- Maybe we are already using Weight Sharing ? **WE DO**\n",
    "- Maybe get better performances ? \n",
    "- Add dropout layer and stuff like this\n",
    "- Add Auxilary losses (also return CNNs results and use them with a loss function, similar to comparisson net) **Done**\n",
    "- Benchmark **Function Done** \n",
    "- Write report **Started** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf0dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import dlc_practical_prologue as prologue\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cbc5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the train and test sets.\n",
    "N = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2738a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model performs each digit classification with 2 different CNNs (so no weight sharing)        \n",
    "class No_Weight_Sharing_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layers that handle digit classification with first CNN\n",
    "        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1_1 = nn.Linear(256, 200)\n",
    "        self.fc2_1 = nn.Linear(200, 10)\n",
    "        \n",
    "        # Layers that handle digit classification with second CNN\n",
    "        self.conv1_2 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2_2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1_2 = nn.Linear(256, 200)\n",
    "        self.fc2_2 = nn.Linear(200, 10)\n",
    "        \n",
    "        # Layers that handle comparisson \n",
    "        self.fc3 = nn.Linear(20, 300)\n",
    "        self.fc4 = nn.Linear(300, 300)\n",
    "        self.fc5 = nn.Linear(300, 2)\n",
    "        \n",
    "    def cnn1(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1_1(x), kernel_size=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_1(x), kernel_size=2))\n",
    "        x = F.relu(self.fc1_1(x.view(-1, 256)))\n",
    "        x = self.fc2_1(x)\n",
    "        return x\n",
    "    \n",
    "    def cnn2(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1_2(x), kernel_size=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_2(x), kernel_size=2))\n",
    "        x = F.relu(self.fc1_2(x.view(-1, 256)))\n",
    "        x = self.fc2_2(x)\n",
    "        return x\n",
    "    \n",
    "    def mlp(self, x):\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        s = x.shape\n",
    "        input_1 = x[:,0,:,:].reshape([s[0],1,s[2],s[3]])\n",
    "        input_2 = x[:,1,:,:].reshape([s[0],1,s[2],s[3]])\n",
    "        \n",
    "        output_1 = self.cnn1(input_1)\n",
    "        output_2 = self.cnn2(input_2)\n",
    "        \n",
    "        concatenated = torch.cat((output_1, output_2), 1)\n",
    "        \n",
    "        comparison = self.mlp(concatenated)\n",
    "        return comparison   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62a0894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition \n",
    "\n",
    "        \n",
    "class Simple_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layers that handle digit classification \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        \n",
    "        # Layers that handle comparisson \n",
    "        self.fc3 = nn.Linear(20, 300)\n",
    "        self.fc4 = nn.Linear(300, 300)\n",
    "        self.fc5 = nn.Linear(300, 2)\n",
    "        \n",
    "    def cnn(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def mlp(self, x):\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        s = x.shape\n",
    "        input_1 = x[:,0,:,:].reshape([s[0],1,s[2],s[3]])\n",
    "        input_2 = x[:,1,:,:].reshape([s[0],1,s[2],s[3]])\n",
    "        \n",
    "        output_1 = self.cnn(input_1)\n",
    "        output_2 = self.cnn(input_2)\n",
    "        \n",
    "        concatenated = torch.cat((output_1, output_2), 1)\n",
    "        \n",
    "        comparison = self.mlp(concatenated)\n",
    "        return comparison   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be195dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple_net(model, train_input, train_target, mini_batch_size, nb_epochs = 100, use_optimizer= None, _print=False):\n",
    "    criterion = nn.MSELoss()\n",
    "    eta = 1e-3\n",
    "    if use_optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=eta)\n",
    "    if use_optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=eta)\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            target = train_target.narrow(0, b, mini_batch_size).reshape(output.shape).float()\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            acc_loss = acc_loss + loss.item()\n",
    " \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            if use_optimizer != None :\n",
    "                optimizer.step()\n",
    "            else :\n",
    "                with torch.no_grad():\n",
    "                    for p in model.parameters():\n",
    "                        p -= eta * p.grad\n",
    "        if _print:\n",
    "            print(e, acc_loss)\n",
    "            \n",
    "def train_model_simple_net_2(model, train_input, train_target, mini_batch_size, nb_epochs = 100, use_optimizer= None, _print=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    eta = 1e-3\n",
    "    if use_optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=eta)\n",
    "    if use_optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=eta)\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            target = train_target.narrow(0, b, mini_batch_size).long()\n",
    "            loss = criterion(output, target)\n",
    "            acc_loss = acc_loss + loss.item()\n",
    " \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            if use_optimizer != None :\n",
    "                optimizer.step()\n",
    "            else :\n",
    "                with torch.no_grad():\n",
    "                    for p in model.parameters():\n",
    "                        p -= eta * p.grad\n",
    "        if _print:\n",
    "            print(e, acc_loss)\n",
    "        \n",
    "def compute_nb_errors_simple_net(model, input, target, mini_batch_size):\n",
    "    nb_errors = 0\n",
    "\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        output = model(input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if target[b + k, predicted_classes[k]] <= 0:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5311438",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_total = Simple_Net()\n",
    "\n",
    "train_target_one_hot = prologue.convert_to_one_hot_labels(train_input, train_target)\n",
    "train_model_simple_net_2(model_total, train_input, train_target, mini_batch_size=250, nb_epochs=25, use_optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "383b2328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 14.90% 149/1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_target_total = prologue.convert_to_one_hot_labels(test_input, test_target)\n",
    "nb_test_errors = compute_nb_errors_simple_net(model_total, test_input, test_target_total, mini_batch_size=250)\n",
    "print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                      nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "489b70d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted : 4 < 8\n",
      "Predicted : 3 < 7\n",
      "Predicted : 3 < 7\n",
      "Predicted : 1 < 4\n",
      "Predicted : 4 < 5\n",
      "Predicted : 4 < 6\n",
      "Predicted : 3 < 7\n",
      "Predicted : 4 > 1\n",
      "Predicted : 5 > 1\n",
      "Predicted : 6 < 5\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    input_to_test = test_input[i]\n",
    "    first_label = test_classes[i][0]\n",
    "    second_label = test_classes[i][1] \n",
    "    s = input_to_test.shape\n",
    "    output = model_total(input_to_test.reshape([1,s[0], s[1], s[2]]))\n",
    "    _, predicted_classes = output.max(1)\n",
    "    print(f\"Predicted : {first_label} {'>' if predicted_classes.item() == 0 else '<'} {second_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "23b4ad6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.6856465339660645\n",
      "1 1.4614585041999817\n",
      "2 1.0191197395324707\n",
      "3 0.9204623401165009\n",
      "4 0.8185340911149979\n",
      "5 0.7011362165212631\n",
      "6 0.6271707266569138\n",
      "7 0.5884476453065872\n",
      "8 0.5331521481275558\n",
      "9 0.4940749928355217\n",
      "10 0.4571880176663399\n",
      "11 0.421296089887619\n",
      "12 0.39193376153707504\n",
      "13 0.35857199132442474\n",
      "14 0.327596090734005\n",
      "15 0.3004740923643112\n",
      "16 0.2747432813048363\n",
      "17 0.2485593818128109\n",
      "18 0.22220168635249138\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-5768890b5d4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     train_model_simple_net(model_total, train_input, train_target_one_hot, mini_batch_size=250, \n\u001b[0;32m---> 17\u001b[0;31m                       nb_epochs=25, use_optimizer=\"adam\")\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Evaluate performances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-30f8cb4a6249>\u001b[0m in \u001b[0;36mtrain_model_simple_net\u001b[0;34m(model, train_input, train_target, mini_batch_size, nb_epochs, use_optimizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda3/envs/IAPR-DL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-7b7066cd1c9d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0minput_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moutput_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0moutput_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-7b7066cd1c9d>\u001b[0m in \u001b[0;36mcnn\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda3/envs/IAPR-DL/lib/python3.7/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda3/envs/IAPR-DL/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     return torch.max_pool2d(\n\u001b[0;32m--> 488\u001b[0;31m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m max_pool2d = boolean_dispatch(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Benchmark of the basic network with Adam optimizer\n",
    "nb_trials = 10\n",
    "N = 1000\n",
    "performances = []\n",
    "for trial in range(nb_trials):\n",
    "    \n",
    "    # Generate Data \n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "    train_target_one_hot = prologue.convert_to_one_hot_labels(train_input, train_target)\n",
    "    test_target_total = prologue.convert_to_one_hot_labels(test_input, test_target)\n",
    "    \n",
    "    # Define the model \n",
    "    model_total = Simple_Net()\n",
    "    \n",
    "    # Train the model\n",
    "    train_model_simple_net(model_total, train_input, train_target_one_hot, mini_batch_size=250, \n",
    "                      nb_epochs=25, use_optimizer=\"adam\")\n",
    "    \n",
    "    # Evaluate performances \n",
    "    nb_test_errors = compute_nb_errors_simple_net(model_total, test_input, test_target_total, mini_batch_size=250)\n",
    "    print('test error Net {:d} {:0.2f}% {:d}/{:d}'.format(trial, (100 * nb_test_errors) / test_input.size(0),\n",
    "                                                          nb_test_errors, test_input.size(0)))\n",
    "    performances.append(nb_test_errors)\n",
    "    \n",
    "mean_perf = 100 * sum(performances) / (N * nb_trials)\n",
    "print(f\"Average precision of this architecture {mean_perf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa518588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Auxiliary_Loss_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layers that handle digit classification \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        \n",
    "        # Layers that handle comparisson \n",
    "        self.fc3 = nn.Linear(20, 300)\n",
    "        self.fc4 = nn.Linear(300, 300)\n",
    "        self.fc5 = nn.Linear(300, 2)\n",
    "        \n",
    "    def cnn(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def mlp(self, x):\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        s = x.shape\n",
    "        \n",
    "        input_1 = x[:,0,:,:].reshape([s[0],1,s[2],s[3]])\n",
    "        input_2 = x[:,1,:,:].reshape([s[0],1,s[2],s[3]])\n",
    "        \n",
    "        output_1 = self.cnn(input_1)\n",
    "        output_2 = self.cnn(input_2)\n",
    "        \n",
    "        concatenated = torch.cat((output_1, output_2), 1)\n",
    "        \n",
    "        comparison = self.mlp(concatenated)\n",
    "        return output_1, output_2, comparison  \n",
    "    \n",
    "def train_model_auxiliary_loss(model, train_input, train_target, train_classes, mini_batch_size, nb_epochs = 100, use_optimizer= None, _print=False):\n",
    "    criterion_auxilary = nn.CrossEntropyLoss()\n",
    "    criterion_final = nn.MSELoss()\n",
    "    \n",
    "    eta = 1e-3\n",
    "    if use_optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=eta)\n",
    "    if use_optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=eta)\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            digit_1, digit_2, comparison = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            target_comparison = train_target.narrow(0, b, mini_batch_size).reshape(comparison.shape).float()\n",
    "            \n",
    "            target_digit_1, target_digit_2 = train_classes.narrow(0, b, mini_batch_size)[:,0], train_classes.narrow(0, b, mini_batch_size)[:,1]\n",
    "            loss1 = criterion_auxilary(digit_1, target_digit_1)\n",
    "            loss2 = criterion_auxilary(digit_2, target_digit_2)\n",
    "            loss3 = criterion_final(comparison, target_comparison)\n",
    "            acc_loss = acc_loss + loss1.item() + loss2.item() + loss3.item()\n",
    " \n",
    "            model.zero_grad()\n",
    "            loss1.backward(retain_graph=True)\n",
    "            loss2.backward(retain_graph=True)\n",
    "            loss3.backward()\n",
    "            \n",
    "            if use_optimizer != None :\n",
    "                optimizer.step()\n",
    "            else :\n",
    "                with torch.no_grad():\n",
    "                    for p in model.parameters():\n",
    "                        p -= eta * p.grad\n",
    "        if _print :\n",
    "            print(e, acc_loss)\n",
    "        \n",
    "def train_model_auxiliary_loss_2(model, train_input, train_target, train_classes, mini_batch_size, nb_epochs = 100, use_optimizer= None, _print=False):\n",
    "    criterion_auxilary = nn.CrossEntropyLoss()\n",
    "    criterion_final = nn.CrossEntropyLoss()\n",
    "    \n",
    "    eta = 1e-3\n",
    "    if use_optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=eta)\n",
    "    if use_optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=eta)\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            digit_1, digit_2, comparison = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            target_comparison = train_target.narrow(0, b, mini_batch_size).long()\n",
    "            \n",
    "            target_digit_1, target_digit_2 = train_classes.narrow(0, b, mini_batch_size)[:,0], train_classes.narrow(0, b, mini_batch_size)[:,1]\n",
    "            loss1 = criterion_auxilary(digit_1, target_digit_1)\n",
    "            loss2 = criterion_auxilary(digit_2, target_digit_2)\n",
    "            loss3 = criterion_final(comparison, target_comparison)\n",
    "            acc_loss = acc_loss + loss1.item() + loss2.item() + loss3.item()\n",
    " \n",
    "            model.zero_grad()\n",
    "            loss1.backward(retain_graph=True)\n",
    "            loss2.backward(retain_graph=True)\n",
    "            loss3.backward()\n",
    "            \n",
    "            if use_optimizer != None :\n",
    "                optimizer.step()\n",
    "            else :\n",
    "                with torch.no_grad():\n",
    "                    for p in model.parameters():\n",
    "                        p -= eta * p.grad\n",
    "        if _print :\n",
    "            print(e, acc_loss)\n",
    "def compute_nb_errors_auxilary_loss(model, input, target, mini_batch_size):\n",
    "    nb_errors = 0\n",
    "\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        _, _, output = model(input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if target[b + k, predicted_classes[k]] <= 0:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab504c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n",
      "torch.Size([250, 10]) torch.Size([250])\n"
     ]
    }
   ],
   "source": [
    "model_auxiliary = Auxiliary_Loss_Net()\n",
    "\n",
    "train_target_one_hot = prologue.convert_to_one_hot_labels(train_input, train_target)\n",
    "train_model_auxiliary_loss(model_auxiliary, train_input, train_target_one_hot, train_classes, mini_batch_size=250, nb_epochs=25, use_optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "761d6df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 9.70% 97/1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_target_total = prologue.convert_to_one_hot_labels(test_input, test_target)\n",
    "nb_test_errors = compute_nb_errors_auxilary_loss(model_auxiliary, test_input, test_target_total, mini_batch_size=250)\n",
    "print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                      nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f2a463cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, train_function, evaluate_function, nb_trials=40, N=1000, mini_batch_size=250, nb_epochs=25, model_requires_target_and_classes=False, one_hot_train_target=True, _print=False):\n",
    "    # Benchmark of the basic network with Adam optimizer\n",
    "    performances = []\n",
    "    for trial in range(nb_trials):\n",
    "\n",
    "        # Generate Data \n",
    "        train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "        if one_hot_train_target:\n",
    "            train_target_one_hot = prologue.convert_to_one_hot_labels(train_input, train_target)\n",
    "        else :\n",
    "            train_target_one_hot = train_target\n",
    "        test_target_one_hot = prologue.convert_to_one_hot_labels(test_input, test_target)\n",
    "\n",
    "        # Define the model \n",
    "        model_total = model()\n",
    "\n",
    "        # Train the model\n",
    "        if model_requires_target_and_classes : \n",
    "            train_function(model_total, train_input, train_target_one_hot, train_classes, mini_batch_size=mini_batch_size,\n",
    "                           nb_epochs=nb_epochs, use_optimizer=\"adam\", _print=_print)\n",
    "        else :\n",
    "            train_function(model_total, train_input, train_target_one_hot, mini_batch_size=mini_batch_size,\n",
    "                           nb_epochs=nb_epochs, use_optimizer=\"adam\", _print=_print)\n",
    "\n",
    "        # Evaluate performances \n",
    "        nb_test_errors = evaluate_function(model_total, test_input, test_target_one_hot, mini_batch_size=mini_batch_size)\n",
    "        print('test error Net trial {:d} {:0.2f}% {:d}/{:d}'.format(trial, (100 * nb_test_errors) / test_input.size(0),\n",
    "                                                              nb_test_errors, test_input.size(0)))\n",
    "        performances.append(nb_test_errors)\n",
    "\n",
    "    mean_perf = 100 * sum(performances) / (N * nb_trials)\n",
    "    print(f\"Average precision of this architecture {mean_perf}%\")\n",
    "    \n",
    "    std_dev = math.sqrt(sum(list(map(lambda x : x - mean_perf,performances))))/nb_trials\n",
    "    print(f\"With standard deviation of  {std_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "469cec97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark of the model with no Weight Sharing\n",
      "Benchmark of the model with no Weight Sharing CrossEntropyLoss\n",
      "test error Net trial 0 16.50% 165/1000\n",
      "test error Net trial 1 16.90% 169/1000\n",
      "test error Net trial 2 20.10% 201/1000\n",
      "test error Net trial 3 16.00% 160/1000\n",
      "test error Net trial 4 24.10% 241/1000\n",
      "test error Net trial 5 16.30% 163/1000\n",
      "test error Net trial 6 18.30% 183/1000\n",
      "test error Net trial 7 14.70% 147/1000\n",
      "test error Net trial 8 16.00% 160/1000\n",
      "test error Net trial 9 17.70% 177/1000\n",
      "test error Net trial 10 16.80% 168/1000\n",
      "test error Net trial 11 15.20% 152/1000\n",
      "test error Net trial 12 17.80% 178/1000\n",
      "test error Net trial 13 17.50% 175/1000\n",
      "test error Net trial 14 16.70% 167/1000\n",
      "test error Net trial 15 16.90% 169/1000\n",
      "test error Net trial 16 19.40% 194/1000\n",
      "test error Net trial 17 25.00% 250/1000\n",
      "test error Net trial 18 18.40% 184/1000\n",
      "test error Net trial 19 15.20% 152/1000\n",
      "test error Net trial 20 15.50% 155/1000\n",
      "test error Net trial 21 15.20% 152/1000\n",
      "test error Net trial 22 17.80% 178/1000\n",
      "test error Net trial 23 17.90% 179/1000\n",
      "test error Net trial 24 17.10% 171/1000\n",
      "test error Net trial 25 17.40% 174/1000\n",
      "test error Net trial 26 19.60% 196/1000\n",
      "test error Net trial 27 18.60% 186/1000\n",
      "test error Net trial 28 19.60% 196/1000\n",
      "test error Net trial 29 15.00% 150/1000\n",
      "test error Net trial 30 17.40% 174/1000\n",
      "test error Net trial 31 17.60% 176/1000\n",
      "test error Net trial 32 17.30% 173/1000\n",
      "test error Net trial 33 18.70% 187/1000\n",
      "test error Net trial 34 19.20% 192/1000\n",
      "test error Net trial 35 18.90% 189/1000\n",
      "test error Net trial 36 25.60% 256/1000\n",
      "test error Net trial 37 16.20% 162/1000\n",
      "test error Net trial 38 16.70% 167/1000\n",
      "test error Net trial 39 28.10% 281/1000\n",
      "Average precision of this architecture 18.1225%\n",
      "With standard deviation of  2.0192975263690087\n"
     ]
    }
   ],
   "source": [
    "print(\"Benchmark of the model with no Weight Sharing\")\n",
    "#benchmark_model(No_Weight_Sharing_Net, train_model_simple_net, compute_nb_errors_simple_net)\n",
    "print(\"Benchmark of the model with no Weight Sharing CrossEntropyLoss\")\n",
    "benchmark_model(No_Weight_Sharing_Net, train_model_simple_net_2, compute_nb_errors_simple_net, one_hot_train_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aa572bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark of the model with Weight Sharing MSE\n",
      "Benchmark of the model with Weight Sharing CrossEntropyLoss\n",
      "test error Net trial 0 15.60% 156/1000\n",
      "test error Net trial 1 15.80% 158/1000\n",
      "test error Net trial 2 16.60% 166/1000\n",
      "test error Net trial 3 18.60% 186/1000\n",
      "test error Net trial 4 18.10% 181/1000\n",
      "test error Net trial 5 16.90% 169/1000\n",
      "test error Net trial 6 15.70% 157/1000\n",
      "test error Net trial 7 16.20% 162/1000\n",
      "test error Net trial 8 14.10% 141/1000\n",
      "test error Net trial 9 17.00% 170/1000\n",
      "test error Net trial 10 13.30% 133/1000\n",
      "test error Net trial 11 15.50% 155/1000\n",
      "test error Net trial 12 14.40% 144/1000\n",
      "test error Net trial 13 16.20% 162/1000\n",
      "test error Net trial 14 17.40% 174/1000\n",
      "test error Net trial 15 17.40% 174/1000\n",
      "test error Net trial 16 15.70% 157/1000\n",
      "test error Net trial 17 16.20% 162/1000\n",
      "test error Net trial 18 13.80% 138/1000\n",
      "test error Net trial 19 14.60% 146/1000\n",
      "test error Net trial 20 12.40% 124/1000\n",
      "test error Net trial 21 17.30% 173/1000\n",
      "test error Net trial 22 16.20% 162/1000\n",
      "test error Net trial 23 18.50% 185/1000\n",
      "test error Net trial 24 17.50% 175/1000\n",
      "test error Net trial 25 14.50% 145/1000\n",
      "test error Net trial 26 13.70% 137/1000\n",
      "test error Net trial 27 15.20% 152/1000\n",
      "test error Net trial 28 13.50% 135/1000\n",
      "test error Net trial 29 13.50% 135/1000\n",
      "test error Net trial 30 17.50% 175/1000\n",
      "test error Net trial 31 15.70% 157/1000\n",
      "test error Net trial 32 15.50% 155/1000\n",
      "test error Net trial 33 14.90% 149/1000\n",
      "test error Net trial 34 17.90% 179/1000\n",
      "test error Net trial 35 17.20% 172/1000\n",
      "test error Net trial 36 14.10% 141/1000\n",
      "test error Net trial 37 16.50% 165/1000\n",
      "test error Net trial 38 13.80% 138/1000\n",
      "test error Net trial 39 16.90% 169/1000\n",
      "Average precision of this architecture 15.785%\n",
      "With standard deviation of  1.8845755490295424\n"
     ]
    }
   ],
   "source": [
    "print(\"Benchmark of the model with Weight Sharing MSE\")\n",
    "#benchmark_model(Simple_Net, train_model_simple_net, compute_nb_errors_simple_net)\n",
    "print(\"Benchmark of the model with Weight Sharing CrossEntropyLoss\")\n",
    "benchmark_model(Simple_Net, train_model_simple_net_2, compute_nb_errors_simple_net, one_hot_train_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "04efb119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark of the model with Weight Sharing and an auxiliary loss MSE\n",
      "Benchmark of the model with Weight Sharing and an auxiliary loss Cross Entropy Loss\n",
      "test error Net trial 0 11.60% 116/1000\n",
      "test error Net trial 1 9.80% 98/1000\n",
      "test error Net trial 2 11.20% 112/1000\n",
      "test error Net trial 3 10.30% 103/1000\n",
      "test error Net trial 4 9.50% 95/1000\n",
      "test error Net trial 5 10.80% 108/1000\n",
      "test error Net trial 6 9.60% 96/1000\n",
      "test error Net trial 7 10.40% 104/1000\n",
      "test error Net trial 8 11.30% 113/1000\n",
      "test error Net trial 9 9.20% 92/1000\n",
      "test error Net trial 10 13.90% 139/1000\n",
      "test error Net trial 11 13.50% 135/1000\n",
      "test error Net trial 12 10.10% 101/1000\n",
      "test error Net trial 13 8.80% 88/1000\n",
      "test error Net trial 14 11.60% 116/1000\n",
      "test error Net trial 15 9.60% 96/1000\n",
      "test error Net trial 16 12.20% 122/1000\n",
      "test error Net trial 17 9.90% 99/1000\n",
      "test error Net trial 18 8.90% 89/1000\n",
      "test error Net trial 19 9.40% 94/1000\n",
      "test error Net trial 20 10.00% 100/1000\n",
      "test error Net trial 21 9.90% 99/1000\n",
      "test error Net trial 22 11.00% 110/1000\n",
      "test error Net trial 23 10.90% 109/1000\n",
      "test error Net trial 24 9.30% 93/1000\n",
      "test error Net trial 25 11.60% 116/1000\n",
      "test error Net trial 26 8.70% 87/1000\n",
      "test error Net trial 27 10.80% 108/1000\n",
      "test error Net trial 28 10.80% 108/1000\n",
      "test error Net trial 29 13.20% 132/1000\n",
      "test error Net trial 30 10.20% 102/1000\n",
      "test error Net trial 31 9.10% 91/1000\n",
      "test error Net trial 32 10.80% 108/1000\n",
      "test error Net trial 33 10.50% 105/1000\n",
      "test error Net trial 34 11.20% 112/1000\n",
      "test error Net trial 35 11.70% 117/1000\n",
      "test error Net trial 36 11.10% 111/1000\n",
      "test error Net trial 37 10.00% 100/1000\n",
      "test error Net trial 38 9.60% 96/1000\n",
      "test error Net trial 39 11.50% 115/1000\n",
      "Average precision of this architecture 10.5875%\n",
      "With standard deviation of  1.5434336720442503\n"
     ]
    }
   ],
   "source": [
    "print(\"Benchmark of the model with Weight Sharing and an auxiliary loss MSE\")\n",
    "#benchmark_model(Auxiliary_Loss_Net, train_model_auxiliary_loss, compute_nb_errors_auxilary_loss, model_requires_target_and_classes=True)\n",
    "print(\"Benchmark of the model with Weight Sharing and an auxiliary loss Cross Entropy Loss\")\n",
    "benchmark_model(Auxiliary_Loss_Net, train_model_auxiliary_loss_2, compute_nb_errors_auxilary_loss, model_requires_target_and_classes=True, one_hot_train_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6914828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Auxiliary_Loss_Net_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layers that handle digit classification \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.dropout_1 = nn.Dropout(p=0.1)\n",
    "        self.dropout_2 = nn.Dropout(p=0.1)\n",
    "        self.dropout_3 = nn.Dropout(p=0.1)\n",
    "        # Layers that handle comparisson \n",
    "        self.fc3 = nn.Linear(20, 300)\n",
    "        self.fc4 = nn.Linear(300, 300)\n",
    "        self.fc5 = nn.Linear(300, 2)\n",
    "        \n",
    "    def cnn(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        x = self.dropout_1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = self.dropout_2(x)\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.dropout_3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def mlp(self, x):\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        s = x.shape\n",
    "        \n",
    "        input_1 = x[:,0,:,:].reshape([s[0],1,s[2],s[3]])\n",
    "        input_2 = x[:,1,:,:].reshape([s[0],1,s[2],s[3]])\n",
    "        \n",
    "        output_1 = self.cnn(input_1)\n",
    "        output_2 = self.cnn(input_2)\n",
    "        \n",
    "        concatenated = torch.cat((output_1, output_2), 1)\n",
    "        \n",
    "        comparison = self.mlp(concatenated)\n",
    "        return output_1, output_2, comparison  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8c8dc356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net trial 0 8.70% 87/1000\n",
      "test error Net trial 1 8.70% 87/1000\n",
      "test error Net trial 2 8.50% 85/1000\n",
      "test error Net trial 3 10.30% 103/1000\n",
      "test error Net trial 4 8.00% 80/1000\n",
      "test error Net trial 5 9.60% 96/1000\n",
      "test error Net trial 6 9.00% 90/1000\n",
      "test error Net trial 7 9.20% 92/1000\n",
      "test error Net trial 8 7.90% 79/1000\n",
      "test error Net trial 9 7.90% 79/1000\n",
      "test error Net trial 10 7.80% 78/1000\n",
      "test error Net trial 11 9.60% 96/1000\n",
      "test error Net trial 12 9.50% 95/1000\n",
      "test error Net trial 13 9.30% 93/1000\n",
      "test error Net trial 14 8.40% 84/1000\n",
      "test error Net trial 15 9.60% 96/1000\n",
      "test error Net trial 16 9.40% 94/1000\n",
      "test error Net trial 17 8.10% 81/1000\n",
      "test error Net trial 18 9.10% 91/1000\n",
      "test error Net trial 19 8.40% 84/1000\n",
      "test error Net trial 20 7.00% 70/1000\n",
      "test error Net trial 21 9.90% 99/1000\n",
      "test error Net trial 22 9.90% 99/1000\n",
      "test error Net trial 23 9.20% 92/1000\n",
      "test error Net trial 24 8.00% 80/1000\n",
      "test error Net trial 25 9.10% 91/1000\n",
      "test error Net trial 26 7.30% 73/1000\n",
      "test error Net trial 27 8.40% 84/1000\n",
      "test error Net trial 28 7.50% 75/1000\n",
      "test error Net trial 29 7.90% 79/1000\n",
      "test error Net trial 30 8.70% 87/1000\n",
      "test error Net trial 31 9.30% 93/1000\n",
      "test error Net trial 32 9.30% 93/1000\n",
      "test error Net trial 33 8.60% 86/1000\n",
      "test error Net trial 34 7.50% 75/1000\n",
      "test error Net trial 35 10.10% 101/1000\n",
      "test error Net trial 36 8.50% 85/1000\n",
      "test error Net trial 37 9.30% 93/1000\n",
      "test error Net trial 38 7.50% 75/1000\n",
      "test error Net trial 39 8.40% 84/1000\n",
      "Average precision of this architecture 8.71%\n",
      "With standard deviation of  1.3999107114384115\n",
      "test error Net trial 0 10.70% 107/1000\n",
      "test error Net trial 1 12.30% 123/1000\n",
      "test error Net trial 2 9.70% 97/1000\n",
      "test error Net trial 3 9.80% 98/1000\n",
      "test error Net trial 4 11.60% 116/1000\n",
      "test error Net trial 5 9.50% 95/1000\n",
      "test error Net trial 6 10.10% 101/1000\n",
      "test error Net trial 7 9.90% 99/1000\n",
      "test error Net trial 8 10.70% 107/1000\n",
      "test error Net trial 9 10.10% 101/1000\n",
      "test error Net trial 10 10.10% 101/1000\n",
      "test error Net trial 11 8.20% 82/1000\n",
      "test error Net trial 12 11.40% 114/1000\n",
      "test error Net trial 13 8.40% 84/1000\n",
      "test error Net trial 14 9.50% 95/1000\n",
      "test error Net trial 15 10.80% 108/1000\n",
      "test error Net trial 16 8.10% 81/1000\n",
      "test error Net trial 17 10.90% 109/1000\n",
      "test error Net trial 18 12.40% 124/1000\n",
      "test error Net trial 19 11.70% 117/1000\n",
      "test error Net trial 20 8.30% 83/1000\n",
      "test error Net trial 21 11.20% 112/1000\n",
      "test error Net trial 22 9.60% 96/1000\n",
      "test error Net trial 23 13.00% 130/1000\n",
      "test error Net trial 24 13.00% 130/1000\n",
      "test error Net trial 25 9.00% 90/1000\n",
      "test error Net trial 26 11.20% 112/1000\n",
      "test error Net trial 27 11.70% 117/1000\n",
      "test error Net trial 28 11.40% 114/1000\n",
      "test error Net trial 29 10.60% 106/1000\n",
      "test error Net trial 30 12.30% 123/1000\n",
      "test error Net trial 31 11.40% 114/1000\n",
      "test error Net trial 32 8.70% 87/1000\n",
      "test error Net trial 33 8.60% 86/1000\n",
      "test error Net trial 34 10.80% 108/1000\n",
      "test error Net trial 35 11.20% 112/1000\n",
      "test error Net trial 36 11.50% 115/1000\n",
      "test error Net trial 37 13.30% 133/1000\n",
      "test error Net trial 38 11.00% 110/1000\n",
      "test error Net trial 39 12.10% 121/1000\n",
      "Average precision of this architecture 10.645%\n",
      "With standard deviation of  1.5476191391941365\n"
     ]
    }
   ],
   "source": [
    "trials = 40\n",
    "benchmark_model(Auxiliary_Loss_Net_2, train_model_auxiliary_loss_2, compute_nb_errors_auxilary_loss, model_requires_target_and_classes=True, one_hot_train_target=False, nb_trials=trials, nb_epochs=50)\n",
    "\n",
    "benchmark_model(Auxiliary_Loss_Net, train_model_auxiliary_loss_2, compute_nb_errors_auxilary_loss, model_requires_target_and_classes=True, one_hot_train_target=False, nb_trials=trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b4459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
